{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":228180,"sourceType":"datasetVersion","datasetId":14872}],"dockerImageVersionId":30170,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* **her below is the code written by kaggle when we click on new notebook in perticular dataset.It is pedifinded code which gives us the address of the dataset.**\n* **In your case either directly use dataset from kaggle or change the path in read_csv() function if you want to use it extenally.**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T12:49:45.791886Z","iopub.execute_input":"2024-05-29T12:49:45.792256Z","iopub.status.idle":"2024-05-29T12:49:45.825921Z","shell.execute_reply.started":"2024-05-29T12:49:45.792158Z","shell.execute_reply":"2024-05-29T12:49:45.824845Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/graduate-admissions/Admission_Predict.csv\n/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:45.827582Z","iopub.execute_input":"2024-05-29T12:49:45.827974Z","iopub.status.idle":"2024-05-29T12:49:45.866725Z","shell.execute_reply.started":"2024-05-29T12:49:45.827934Z","shell.execute_reply":"2024-05-29T12:49:45.865789Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n0           1        337          118                  4  4.5   4.5  9.65   \n1           2        324          107                  4  4.0   4.5  8.87   \n2           3        316          104                  3  3.0   3.5  8.00   \n3           4        322          110                  3  3.5   2.5  8.67   \n4           5        314          103                  2  2.0   3.0  8.21   \n\n   Research  Chance of Admit   \n0         1              0.92  \n1         1              0.76  \n2         1              0.72  \n3         1              0.80  \n4         0              0.65  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Serial No.</th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=df.drop(columns=['Serial No.'])\n\n#drop serial number column as it is not important ","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:45.871265Z","iopub.execute_input":"2024-05-29T12:49:45.871511Z","iopub.status.idle":"2024-05-29T12:49:45.882151Z","shell.execute_reply.started":"2024-05-29T12:49:45.871484Z","shell.execute_reply":"2024-05-29T12:49:45.881142Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:45.892489Z","iopub.execute_input":"2024-05-29T12:49:45.892827Z","iopub.status.idle":"2024-05-29T12:49:45.908626Z","shell.execute_reply.started":"2024-05-29T12:49:45.892788Z","shell.execute_reply":"2024-05-29T12:49:45.907579Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n0        337          118                  4  4.5   4.5  9.65         1   \n1        324          107                  4  4.0   4.5  8.87         1   \n2        316          104                  3  3.0   3.5  8.00         1   \n3        322          110                  3  3.5   2.5  8.67         1   \n4        314          103                  2  2.0   3.0  8.21         0   \n\n   Chance of Admit   \n0              0.92  \n1              0.76  \n2              0.72  \n3              0.80  \n4              0.65  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()\n\n#by info() function we can check charecteristics of our dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:45.929051Z","iopub.execute_input":"2024-05-29T12:49:45.929784Z","iopub.status.idle":"2024-05-29T12:49:45.950158Z","shell.execute_reply.started":"2024-05-29T12:49:45.929715Z","shell.execute_reply":"2024-05-29T12:49:45.948956Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 500 entries, 0 to 499\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   GRE Score          500 non-null    int64  \n 1   TOEFL Score        500 non-null    int64  \n 2   University Rating  500 non-null    int64  \n 3   SOP                500 non-null    float64\n 4   LOR                500 non-null    float64\n 5   CGPA               500 non-null    float64\n 6   Research           500 non-null    int64  \n 7   Chance of Admit    500 non-null    float64\ndtypes: float64(4), int64(4)\nmemory usage: 31.4 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.duplicated().sum()\n\n#here we check our dataset contain any duplicate row or column or not","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:45.991950Z","iopub.execute_input":"2024-05-29T12:49:45.992266Z","iopub.status.idle":"2024-05-29T12:49:46.002559Z","shell.execute_reply.started":"2024-05-29T12:49:45.992236Z","shell.execute_reply":"2024-05-29T12:49:46.001580Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"x=df.iloc[:,:-1]\nx.head()\n\n#input columns","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:46.008366Z","iopub.execute_input":"2024-05-29T12:49:46.008638Z","iopub.status.idle":"2024-05-29T12:49:46.024786Z","shell.execute_reply.started":"2024-05-29T12:49:46.008608Z","shell.execute_reply":"2024-05-29T12:49:46.023808Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n0        337          118                  4  4.5   4.5  9.65         1\n1        324          107                  4  4.0   4.5  8.87         1\n2        316          104                  3  3.0   3.5  8.00         1\n3        322          110                  3  3.5   2.5  8.67         1\n4        314          103                  2  2.0   3.0  8.21         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y=df.iloc[:,-1]\ny.head()\n\n#output columns","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:46.042651Z","iopub.execute_input":"2024-05-29T12:49:46.043019Z","iopub.status.idle":"2024-05-29T12:49:46.050978Z","shell.execute_reply.started":"2024-05-29T12:49:46.042978Z","shell.execute_reply":"2024-05-29T12:49:46.050089Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0    0.92\n1    0.76\n2    0.72\n3    0.80\n4    0.65\nName: Chance of Admit , dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"# Scalling","metadata":{}},{"cell_type":"markdown","source":"* **In simple word scalling means we devide whole data with appropriate number to reduse number value.This appropriate number is selected by scalling class itself.**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:46.074586Z","iopub.execute_input":"2024-05-29T12:49:46.075460Z","iopub.status.idle":"2024-05-29T12:49:47.102788Z","shell.execute_reply.started":"2024-05-29T12:49:46.075412Z","shell.execute_reply":"2024-05-29T12:49:47.101683Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"* when we know about upper bound and lower bound of any column then it is peferable to use MinMaxScaler class instead of StanderdScaler class for scalling.","metadata":{}},{"cell_type":"code","source":"x=scaler.fit_transform(x)\nx\n\n#scalling of x","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:47.104871Z","iopub.execute_input":"2024-05-29T12:49:47.105216Z","iopub.status.idle":"2024-05-29T12:49:47.118034Z","shell.execute_reply.started":"2024-05-29T12:49:47.105173Z","shell.execute_reply":"2024-05-29T12:49:47.116836Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([[0.94      , 0.92857143, 0.75      , ..., 0.875     , 0.91346154,\n        1.        ],\n       [0.68      , 0.53571429, 0.75      , ..., 0.875     , 0.66346154,\n        1.        ],\n       [0.52      , 0.42857143, 0.5       , ..., 0.625     , 0.38461538,\n        1.        ],\n       ...,\n       [0.8       , 1.        , 1.        , ..., 1.        , 0.88461538,\n        1.        ],\n       [0.44      , 0.39285714, 0.75      , ..., 1.        , 0.5224359 ,\n        0.        ],\n       [0.74      , 0.75      , 0.75      , ..., 0.875     , 0.71794872,\n        0.        ]])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n\n#seperate training data and testing data","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:47.119556Z","iopub.execute_input":"2024-05-29T12:49:47.120238Z","iopub.status.idle":"2024-05-29T12:49:47.183459Z","shell.execute_reply.started":"2024-05-29T12:49:47.120183Z","shell.execute_reply":"2024-05-29T12:49:47.182622Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:47.185230Z","iopub.execute_input":"2024-05-29T12:49:47.185525Z","iopub.status.idle":"2024-05-29T12:49:47.192150Z","shell.execute_reply.started":"2024-05-29T12:49:47.185491Z","shell.execute_reply":"2024-05-29T12:49:47.191074Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(400, 7)\n(100, 7)\n(400,)\n(100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense\n\n#when want to train model sequentially then we use Sequential class\n#Dense class is used to establish fully connected network","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:47.193763Z","iopub.execute_input":"2024-05-29T12:49:47.194060Z","iopub.status.idle":"2024-05-29T12:49:53.426131Z","shell.execute_reply.started":"2024-05-29T12:49:47.194026Z","shell.execute_reply":"2024-05-29T12:49:53.424971Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\n\nmodel.add(Dense(7,activation='relu',input_dim=7))\nmodel.add(Dense(7,activation='relu'))\nmodel.add(Dense(1,activation='linear'))\n\n#model architecture\n#7 node/neurons in oth layer\n#7 node in 1st layer\n#1 node in 2nd layer\n\n#if we solve regrsssion problem then either we use linear activaion fuction in the last layer","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:53.427378Z","iopub.execute_input":"2024-05-29T12:49:53.427701Z","iopub.status.idle":"2024-05-29T12:49:53.545490Z","shell.execute_reply.started":"2024-05-29T12:49:53.427655Z","shell.execute_reply":"2024-05-29T12:49:53.544567Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.summary()\n\n#see model summary","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:53.546633Z","iopub.execute_input":"2024-05-29T12:49:53.546905Z","iopub.status.idle":"2024-05-29T12:49:53.554058Z","shell.execute_reply.started":"2024-05-29T12:49:53.546873Z","shell.execute_reply":"2024-05-29T12:49:53.552544Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 7)                 56        \n_________________________________________________________________\ndense_1 (Dense)              (None, 7)                 56        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 8         \n=================================================================\nTotal params: 120\nTrainable params: 120\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='mean_squared_error',optimizer='Adam',metrics=['accuracy'])\n\n#when we doing regrssion then loss should be mean_squared_error\n#if select optimizer as Adam\n#metrics=['accuracy']\n#when I want to show accuracy along with loss during training the I have to pass dictionary like this","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:53.555436Z","iopub.execute_input":"2024-05-29T12:49:53.555713Z","iopub.status.idle":"2024-05-29T12:49:53.576551Z","shell.execute_reply.started":"2024-05-29T12:49:53.555677Z","shell.execute_reply":"2024-05-29T12:49:53.575679Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x_train,y_train,epochs=100,validation_split=0.2)\n\n#model training and store model in history","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:49:53.577758Z","iopub.execute_input":"2024-05-29T12:49:53.578008Z","iopub.status.idle":"2024-05-29T12:50:00.668188Z","shell.execute_reply.started":"2024-05-29T12:49:53.577976Z","shell.execute_reply":"2024-05-29T12:50:00.667240Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 1s 25ms/step - loss: 0.5338 - accuracy: 0.0000e+00 - val_loss: 0.5320 - val_accuracy: 0.0000e+00\nEpoch 2/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.5033 - accuracy: 0.0000e+00 - val_loss: 0.5035 - val_accuracy: 0.0000e+00\nEpoch 3/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.4700 - accuracy: 0.0000e+00 - val_loss: 0.4656 - val_accuracy: 0.0000e+00\nEpoch 4/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\nEpoch 5/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.3753 - accuracy: 0.0000e+00 - val_loss: 0.3551 - val_accuracy: 0.0000e+00\nEpoch 6/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.3142 - accuracy: 0.0000e+00 - val_loss: 0.2924 - val_accuracy: 0.0000e+00\nEpoch 7/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.2518 - accuracy: 0.0000e+00 - val_loss: 0.2306 - val_accuracy: 0.0000e+00\nEpoch 8/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.1917 - accuracy: 0.0000e+00 - val_loss: 0.1698 - val_accuracy: 0.0000e+00\nEpoch 9/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.1342 - accuracy: 0.0000e+00 - val_loss: 0.1118 - val_accuracy: 0.0000e+00\nEpoch 10/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0813 - accuracy: 0.0000e+00 - val_loss: 0.0580 - val_accuracy: 0.0000e+00\nEpoch 11/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\nEpoch 12/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.0000e+00 - val_loss: 0.0064 - val_accuracy: 0.0000e+00\nEpoch 13/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.0000e+00 - val_loss: 0.0071 - val_accuracy: 0.0000e+00\nEpoch 14/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 0.0000e+00 - val_loss: 0.0061 - val_accuracy: 0.0000e+00\nEpoch 15/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0062 - val_accuracy: 0.0000e+00\nEpoch 16/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 0.0000e+00 - val_loss: 0.0063 - val_accuracy: 0.0000e+00\nEpoch 17/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0071 - accuracy: 0.0000e+00 - val_loss: 0.0061 - val_accuracy: 0.0000e+00\nEpoch 18/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0059 - val_accuracy: 0.0000e+00\nEpoch 19/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 0.0000e+00 - val_loss: 0.0058 - val_accuracy: 0.0000e+00\nEpoch 20/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 0.0000e+00 - val_loss: 0.0058 - val_accuracy: 0.0000e+00\nEpoch 21/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 0.0000e+00 - val_loss: 0.0058 - val_accuracy: 0.0000e+00\nEpoch 22/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 0.0000e+00 - val_loss: 0.0057 - val_accuracy: 0.0000e+00\nEpoch 23/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 0.0000e+00 - val_loss: 0.0056 - val_accuracy: 0.0000e+00\nEpoch 24/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.0000e+00 - val_loss: 0.0056 - val_accuracy: 0.0000e+00\nEpoch 25/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.0000e+00 - val_loss: 0.0055 - val_accuracy: 0.0000e+00\nEpoch 26/100\n10/10 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - val_loss: 0.0054 - val_accuracy: 0.0000e+00\nEpoch 27/100\n10/10 [==============================] - 0s 11ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0053 - val_accuracy: 0.0000e+00\nEpoch 28/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0060 - accuracy: 0.0000e+00 - val_loss: 0.0053 - val_accuracy: 0.0000e+00\nEpoch 29/100\n10/10 [==============================] - 0s 11ms/step - loss: 0.0060 - accuracy: 0.0000e+00 - val_loss: 0.0052 - val_accuracy: 0.0000e+00\nEpoch 30/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0051 - val_accuracy: 0.0000e+00\nEpoch 31/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0058 - accuracy: 0.0000e+00 - val_loss: 0.0050 - val_accuracy: 0.0000e+00\nEpoch 32/100\n10/10 [==============================] - 0s 10ms/step - loss: 0.0058 - accuracy: 0.0000e+00 - val_loss: 0.0049 - val_accuracy: 0.0000e+00\nEpoch 33/100\n10/10 [==============================] - 0s 11ms/step - loss: 0.0057 - accuracy: 0.0000e+00 - val_loss: 0.0049 - val_accuracy: 0.0000e+00\nEpoch 34/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0057 - accuracy: 0.0000e+00 - val_loss: 0.0048 - val_accuracy: 0.0000e+00\nEpoch 35/100\n10/10 [==============================] - 0s 10ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0048 - val_accuracy: 0.0000e+00\nEpoch 36/100\n10/10 [==============================] - 0s 10ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0047 - val_accuracy: 0.0000e+00\nEpoch 37/100\n10/10 [==============================] - 0s 12ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0047 - val_accuracy: 0.0000e+00\nEpoch 38/100\n10/10 [==============================] - 0s 11ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0046 - val_accuracy: 0.0000e+00\nEpoch 39/100\n10/10 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0046 - val_accuracy: 0.0000e+00\nEpoch 40/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0046 - val_accuracy: 0.0000e+00\nEpoch 41/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0045 - val_accuracy: 0.0000e+00\nEpoch 42/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0045 - val_accuracy: 0.0000e+00\nEpoch 43/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0045 - val_accuracy: 0.0000e+00\nEpoch 44/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\nEpoch 45/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\nEpoch 46/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\nEpoch 47/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\nEpoch 48/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\nEpoch 49/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 0.0043 - val_accuracy: 0.0000e+00\nEpoch 50/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 0.0043 - val_accuracy: 0.0000e+00\nEpoch 51/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 0.0043 - val_accuracy: 0.0000e+00\nEpoch 52/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 0.0000e+00\nEpoch 53/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 0.0000e+00\nEpoch 54/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 0.0000e+00\nEpoch 55/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\nEpoch 56/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\nEpoch 57/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\nEpoch 58/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\nEpoch 59/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\nEpoch 60/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\nEpoch 61/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\nEpoch 62/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\nEpoch 63/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\nEpoch 64/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\nEpoch 65/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\nEpoch 66/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\nEpoch 67/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\nEpoch 68/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\nEpoch 69/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\nEpoch 70/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 71/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 72/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 73/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 74/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 75/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 76/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 77/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 78/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 79/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 80/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\nEpoch 81/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 82/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 83/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 84/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 85/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 86/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 87/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 88/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 89/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\nEpoch 90/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 91/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 92/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 93/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 94/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 95/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 96/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 97/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 98/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 99/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\nEpoch 100/100\n10/10 [==============================] - 0s 6ms/step - loss: 0.0044 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* validation_split = 0.2\n* it is one technique where we take part of data from training data and while traning (while training goes on) we done testing on that data. \n* performance of this type of testing is measured by val_loss and val_accuracy","metadata":{}},{"cell_type":"code","source":"y_predict=model.predict(x_test)\ny_predict\n\n#model testing","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:50:00.672114Z","iopub.execute_input":"2024-05-29T12:50:00.672397Z","iopub.status.idle":"2024-05-29T12:50:00.833801Z","shell.execute_reply.started":"2024-05-29T12:50:00.672350Z","shell.execute_reply":"2024-05-29T12:50:00.832795Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([[0.81115234],\n       [0.6563198 ],\n       [0.5528773 ],\n       [0.84276366],\n       [0.5557243 ],\n       [0.63049096],\n       [0.8012767 ],\n       [0.6920531 ],\n       [0.95566297],\n       [0.81630564],\n       [0.7970903 ],\n       [0.69468755],\n       [0.98591566],\n       [0.8832588 ],\n       [0.85832506],\n       [0.54400474],\n       [0.9263804 ],\n       [0.507434  ],\n       [0.7304063 ],\n       [0.54548216],\n       [0.7390367 ],\n       [0.6234673 ],\n       [0.6482101 ],\n       [0.7337787 ],\n       [0.76158285],\n       [0.85067266],\n       [0.5594905 ],\n       [0.8170713 ],\n       [0.48964587],\n       [0.9911304 ],\n       [0.55733794],\n       [0.71874726],\n       [0.84137774],\n       [0.9574826 ],\n       [0.601795  ],\n       [0.85871136],\n       [0.7981268 ],\n       [0.93605185],\n       [0.92143786],\n       [0.59375846],\n       [0.62409   ],\n       [0.68205863],\n       [0.5979837 ],\n       [0.46159583],\n       [0.5978205 ],\n       [0.81042147],\n       [0.65927106],\n       [0.75487167],\n       [0.90163803],\n       [0.6314666 ],\n       [0.7179165 ],\n       [0.53775465],\n       [0.7600042 ],\n       [0.7240008 ],\n       [0.70789313],\n       [0.69376594],\n       [0.6979723 ],\n       [0.8114536 ],\n       [0.52479666],\n       [0.74473745],\n       [0.58779323],\n       [0.70658565],\n       [0.80623895],\n       [0.6395918 ],\n       [0.76150674],\n       [0.9250811 ],\n       [0.71190524],\n       [0.6212618 ],\n       [0.7241008 ],\n       [0.71265674],\n       [0.8156558 ],\n       [0.6605477 ],\n       [0.48516467],\n       [0.6691137 ],\n       [0.66276526],\n       [0.6160496 ],\n       [0.77089   ],\n       [0.6296418 ],\n       [0.6454964 ],\n       [0.6871795 ],\n       [0.80685455],\n       [0.67026496],\n       [0.7155472 ],\n       [0.86115706],\n       [0.5256047 ],\n       [0.7104799 ],\n       [0.8101405 ],\n       [0.8122069 ],\n       [0.49487212],\n       [0.9056545 ],\n       [0.637532  ],\n       [0.7998759 ],\n       [0.6473308 ],\n       [0.60239387],\n       [0.6516247 ],\n       [0.80310875],\n       [0.6705817 ],\n       [0.7687354 ],\n       [0.72025436],\n       [0.5887862 ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test,y_predict)\n\n#when we do regression task then we use r2_score as the measure of our model","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:50:00.835012Z","iopub.execute_input":"2024-05-29T12:50:00.835262Z","iopub.status.idle":"2024-05-29T12:50:00.842721Z","shell.execute_reply.started":"2024-05-29T12:50:00.835231Z","shell.execute_reply":"2024-05-29T12:50:00.841800Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0.7416641065423375"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\n#plotting graph between training loss and epochs, validation loss and epochs","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:50:00.844373Z","iopub.execute_input":"2024-05-29T12:50:00.844643Z","iopub.status.idle":"2024-05-29T12:50:01.089955Z","shell.execute_reply.started":"2024-05-29T12:50:00.844610Z","shell.execute_reply":"2024-05-29T12:50:01.089032Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x79225868f2d0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\n\n#making graph between training accuracy v/s epochs and validation accuracy and epochs","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:50:01.091142Z","iopub.execute_input":"2024-05-29T12:50:01.091377Z","iopub.status.idle":"2024-05-29T12:50:01.249595Z","shell.execute_reply.started":"2024-05-29T12:50:01.091347Z","shell.execute_reply":"2024-05-29T12:50:01.248593Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x79225ae41e90>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}